{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, tqdm, numbers, math, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/workspace/oct_ca_seg/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch as t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastai.vision import get_files\n",
    "import PIL.Image as pil\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.cm as cm\n",
    "import seg_model.src.model as m\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_path = Path('/workspace/oct_ca_seg/dummy_data_for_script/')\n",
    "data_out_path = Path('/workspace/oct_ca_seg/dummy_data_for_script/dummy_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_out_path):\n",
    "    os.mkdir(data_out_path)\n",
    "elif len(os.listdir(data_out_path)) != 0:\n",
    "    shutil.rmtree(data_out_path)\n",
    "    os.mkdir(data_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#data set stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seg_model.nbs.useful.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def normalize(octimage):\n",
    "    means = octimage.view(3,-1).mean(-1)\n",
    "    stds = octimage.view(3,-1).std(-1)\n",
    "    return (octimage - means[:,None,None])/stds[:,None,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#dataset class\n",
    "class OCTScriptDataset(Dataset):\n",
    "    \"\"\"\n",
    "    First we create a dataset that will encapsulate our data. It has 3 special \n",
    "    functions which will be explained as they go. We will pass this dataset object\n",
    "    to the torch dataloader object later which will make training easier.\n",
    "    \"\"\"\n",
    "    def __init__ (self,\n",
    "                  main_data_dir,\n",
    "                  start_size,\n",
    "                  cropped_size,\n",
    "                  transform,\n",
    "                  input_images,\n",
    "                  cuda):\n",
    "        self.main_data_dir = main_data_dir\n",
    "        self.start_size = start_size\n",
    "        self.transform = transform\n",
    "        self.cropped_size = cropped_size\n",
    "        self.input_images = input_images\n",
    "        self.cuda = cuda\n",
    "        self.name_list = get_files(main_data_dir)\n",
    "        \n",
    "    def visualise(self, idx):\n",
    "        \n",
    "        sample = self.__getitem__(idx)\n",
    "        input_data = sample['input'].cpu().numpy()[0,:,:]\n",
    "        #l_data = sample['label'].cpu().numpy()[0,:,:]\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        f, (axin, axl, ax1comb) = plt.subplots(1,3, sharey=True)\n",
    "        f.subplots_adjust(hspace=0.3)\n",
    "        plt.tight_layout()\n",
    "        \"\"\"\n",
    "        #plot image\n",
    "        \n",
    "        f, ax =  plt.subplots(1,1)\n",
    "        image = ax.imshow(input_data,\n",
    "                            aspect = 'equal')\n",
    "        \n",
    "        f.colorbar(image, ax=ax, orientation='vertical', fraction = 0.05)\n",
    "        \"\"\"\n",
    "        axl.imshow(l_data,\n",
    "                   aspect = 'equal')\n",
    "        \n",
    "        combined = input_data + 10 * l_data \n",
    "        \"\"\"\n",
    "        \n",
    "        #ax1comb.imshow(combined, aspect = 'equal')\n",
    "        #plt.show()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"This function will allow us to index the data object and it will \n",
    "        return a sample.\"\"\"\n",
    "        name = self.name_list[idx]\n",
    "        \n",
    "        #load data  \n",
    "        #label = np.array(pil.open(name.parent.parent/str('labels/' + name.name)))\n",
    "        #print(Path('/workspace/oct_ca_seg/data_oct/labels/')/str(name))\n",
    "        #print(label.shape)\n",
    "        \n",
    "        image = np.array(pil.open(name))\n",
    "        \n",
    "        image = image.astype(float)\n",
    "        #label = label.astype(float)\n",
    "        #print(image.shape)\n",
    "        \n",
    "        #print(label.max())\n",
    "        #print(Image.shape)\n",
    "        if self.transform:\n",
    "            \n",
    "            ysize = self.start_size[0] + 20\n",
    "            xsize = self.start_size[1] + 20\n",
    "            image = skitransforms.resize(image, output_shape=(ysize, xsize))\n",
    "            label = skitransforms.resize(label, output_shape=(ysize, xsize))\n",
    "            \n",
    "            \n",
    "            #print(label.shape)\n",
    "            #print(label.max())\n",
    "            image, label = self.rcrop(image, label)\n",
    "            #print(label.max())\n",
    "            \n",
    "            if self.phflip>0.5:\n",
    "                #hflip\n",
    "                image = np.flip(image, 1)\n",
    "                label = np.flip(label, 1)    \n",
    "                #print(label.max())\n",
    "            #print(label.shape)\n",
    "            \n",
    "            if self.pvflip>0.5:\n",
    "                #vflip\n",
    "                image = np.flip(image, 0)\n",
    "                label = np.flip(label, 0)\n",
    "                #print(label.max())\n",
    "            #print(label.shape)\n",
    "            \n",
    "            angle = np.random.randint(0,360)\n",
    "            image = skitransforms.rotate(image, angle=angle, mode='reflect')\n",
    "            label = skitransforms.rotate(label, angle=angle, mode='reflect')\n",
    "            #print(label.max())\n",
    "            #print(label.shape)\n",
    "            \n",
    "            if np.random.rand() > 0.9:\n",
    "                image = self.spnoise(image)\n",
    "            \n",
    "            if np.random.rand() > 0.5:\n",
    "                image = gaussian(image, sigma=1, mode='reflect')\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            image = skitransforms.resize(image, output_shape=self.start_size)\n",
    "            #label = skitransforms.resize(label, output_shape= self.start_size)\n",
    "        \n",
    "        #image = np.expand_dims(preprocessing.scale(image[:,:,0]), -1)\n",
    "        \n",
    "        #label = np.transpose(label.copy(), (2, 0, 1))\n",
    "        image = np.transpose(image.copy(), (2, 0, 1))\n",
    "        #og = preprocessing.MinMaxScaler(og)\n",
    "        \n",
    "        #label = torch.tensor(label).float()\n",
    "        \n",
    "        #label = torch.gt(label.sum(dim=0).unsqueeze(0), 200).float()\n",
    "        \n",
    "        image = torch.tensor(image).float()\n",
    "        #print(image.size(), label.shape)\n",
    "        image = normalize(image)\n",
    "        \n",
    "        \n",
    "        sample = {'input': image[self.input_images],\n",
    "                  'case_name': [str(name.name)]}\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):    \n",
    "        \"\"\"This function is mandated by Pytorch and allows us to see how many \n",
    "        data points we have in our dataset\"\"\"\n",
    "        return len(self.name_list)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = OCTScriptDataset(data_in_path,(256,256),(256,256),transform=False,input_images=[0,1,2], cuda=False)\n",
    "data_loader = DataLoader(data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.visualise(8), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "opts = {'activation': 'relu',\n",
    "'batch_size': 2,\n",
    "'c_size': (256, 256),\n",
    "'cuda' : True,\n",
    "'dataroot': '/workspace/oct_ca_seg/data_oct',\n",
    "'start_size': (256, 256),\n",
    "'device': 'cuda',\n",
    "'input_images': [0,1,2],\n",
    "'transforms':False,      \n",
    "'dims1': 24,\n",
    "'dims2': 32,\n",
    "'dims3': 48,\n",
    "'epochs': 40,\n",
    "'f1dims': 32,\n",
    "'f1maps': 2,\n",
    "'f2dims': 16,\n",
    "'f2maps': 1,\n",
    "'inputchannels': 3,\n",
    "'normalization': 'batch',\n",
    "'primdims': 16,\n",
    "'primmaps': 4,\n",
    "'reconchannels': 1,\n",
    "'maps1': 8,\n",
    "'maps2': 16,\n",
    "'maps3': 24,\n",
    "'uptype': 'deconv',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "optsagain = {'activation': 'relu',\n",
    "'batch_size': 2,\n",
    "'c_size': (256, 256),\n",
    "'comet': False,\n",
    "'dataroot': '/workspace/oct_ca_seg/data_oct',\n",
    "'device': 'cuda',\n",
    "'dims1': 24,\n",
    "'dims2': 32,\n",
    "'dims3': 48,\n",
    "'epochs': 40,\n",
    "'f1dims': 32,\n",
    "'f1maps': 2,\n",
    "'f2dims': 16,\n",
    "'f2maps': 2,\n",
    "'inputchannels': 3,\n",
    "'la': 0.1,\n",
    "'lb': 1,\n",
    "'lc': 0.05,\n",
    "'loadcheckpoint': '/workspace/oct_ca_seg/runsaves/Final1-pawsey/checkpoints/checkpoint.pt',\n",
    "'location': 'pawsey',\n",
    "'logging': True,\n",
    "'lr': 0.0008,\n",
    "'maps1': 8,\n",
    "'maps2': 16,\n",
    "'maps3': 24,\n",
    "'nThreads': 8,\n",
    "'name': 'Final1-pawsey',\n",
    "'normalization': 'batch',\n",
    "'primdims': 16,\n",
    "'primmaps': 4,\n",
    "'reconchannels': 1,\n",
    "'runsaves_dir': '/group/pawsey0271/abalaji/projects/oct_ca_seg/run_saves',\n",
    "'save': True,\n",
    "'sgamma': 0.8,\n",
    "'sstep': 50,\n",
    "'start_size': (256, 256),\n",
    "'test': True,\n",
    "'train': True,\n",
    "'transforms': True,\n",
    "'uptype': 'deconv',\n",
    "'val': True,\n",
    "'verbose': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options():\n",
    "    def __init__(self, dictionary):\n",
    "        \n",
    "        for k,v in dictionary.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = Options(opts)\n",
    "optsagain = Options(optsagain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = m.CapsNet(opts);\n",
    "model.to(opts.device);\n",
    "\n",
    "modelfake = m.CapsNet(optsagain);\n",
    "modelfake.to(opts.device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class GaussianSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply gaussian smoothing on a\n",
    "    1d, 2d or 3d tensor. Filtering is performed seperately for each channel\n",
    "    in the input using a depthwise convolution.\n",
    "    Arguments:\n",
    "        channels (int, sequence): Number of channels of the input tensors. Output will\n",
    "            have this number of channels as well.\n",
    "        kernel_size (int, sequence): Size of the gaussian kernel.\n",
    "        sigma (float, sequence): Standard deviation of the gaussian kernel.\n",
    "        dim (int, optional): The number of dimensions of the data.\n",
    "            Default value is 2 (spatial).\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, kernel_size, sigma, dim=2):\n",
    "        super(GaussianSmoothing, self).__init__()\n",
    "        if isinstance(kernel_size, numbers.Number):\n",
    "            kernel_size = [kernel_size] * dim\n",
    "        if isinstance(sigma, numbers.Number):\n",
    "            sigma = [sigma] * dim\n",
    "\n",
    "        # The gaussian kernel is the product of the\n",
    "        # gaussian function of each dimension.\n",
    "        kernel = 1\n",
    "        meshgrids = torch.meshgrid(\n",
    "            [\n",
    "                torch.arange(size, dtype=torch.float32)\n",
    "                for size in kernel_size\n",
    "            ]\n",
    "        )\n",
    "        for size, std, mgrid in zip(kernel_size, sigma, meshgrids):\n",
    "            mean = (size - 1) / 2\n",
    "            kernel *= 1 / (std * math.sqrt(2 * math.pi)) * \\\n",
    "                      torch.exp(-((mgrid - mean) / std) ** 2 / 2)\n",
    "\n",
    "        # Make sure sum of values in gaussian kernel equals 1.\n",
    "        kernel = kernel / torch.sum(kernel)\n",
    "\n",
    "        # Reshape to depthwise convolutional weight\n",
    "        kernel = kernel.view(1, 1, *kernel.size())\n",
    "        kernel = kernel.repeat(channels, *[1] * (kernel.dim() - 1))\n",
    "\n",
    "        self.register_buffer('weight', kernel)\n",
    "        self.groups = channels\n",
    "\n",
    "        if dim == 1:\n",
    "            self.conv = F.conv1d\n",
    "        elif dim == 2:\n",
    "            self.conv = F.conv2d\n",
    "        elif dim == 3:\n",
    "            self.conv = F.conv3d\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                'Only 1, 2 and 3 dimensions are supported. Received {}.'.format(dim)\n",
    "            )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Apply gaussian filter to input.\n",
    "        Arguments:\n",
    "            input (torch.Tensor): Input to apply gaussian filter on.\n",
    "        Returns:\n",
    "            filtered (torch.Tensor): Filtered output.\n",
    "        \"\"\"\n",
    "        return self.conv(input, weight=self.weight, groups=self.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class GaussianAndResizing(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, sigma, dim=2):\n",
    "        super(GaussianAndResizing, self).__init__()\n",
    "        self.smooth = GaussianSmoothing(channels, kernel_size, sigma, dim)\n",
    "    \n",
    "    def smoothing(self, x):\n",
    "        xs = x.size()\n",
    "        #print(x.size())\n",
    "        x = x.permute(0,1,4,2,3).reshape([xs[0], int(xs[1]*xs[4]), xs[2], xs[3]])\n",
    "        #print(x.size())\n",
    "        x = self.smooth(x)\n",
    "        x=F.pad(x, (2,0,2,0), 'reflect')\n",
    "        #print(x.size())\n",
    "        x = x.view([xs[0], int(xs[1]), int(xs[4]), xs[2], xs[3]]).permute(0,1,3,4,2)\n",
    "        return x\n",
    "    \n",
    "    def forward(self,x):\n",
    "        #print(self.smoothing(x).size())\n",
    "        return self.smoothing(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Gaussian filter inbetween last two layers to get rid of grid artefacts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_abstract_caps_final2 = nn.Sequential(modelfake.get_abstract_caps_final2, GaussianAndResizing(32, 3, 2).to(opts.device))\n",
    "model.get_abstract_caps_final1 = nn.Sequential(modelfake.get_abstract_caps_final1, GaussianAndResizing(64, 3, 2).to(opts.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(Path('/workspace/oct_ca_seg/runsaves/capstunedarj1.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del modelfake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:10,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 4.59 s, total: 1min 5s\n",
      "Wall time: 10.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "model.eval()\n",
    "for i, sample in tqdm.tqdm(enumerate(data_loader)): \n",
    "    xb, name = sample['input'], sample['case_name'][0][0]\n",
    "    pred = torch.argmin(model(xb)[0], dim=1)\n",
    "    pred = np.array(pred.data[0])\n",
    "    plt.imsave(data_out_path/name, pred, cmap=cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
