{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Capsnet OCT classifier - Arjun Balaji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, os, sys, time\n",
    "import model as m\n",
    "import dataset as d\n",
    "from options import OptionsHome\n",
    "from oct_dataset import RandomCrop, get_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = o.OptionsHome()\n",
    "#opts.parse()\n",
    "#opts.start_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSingleImageCrop(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "        \n",
    "        \n",
    "    def __call__(self, image):\n",
    "\n",
    "        \n",
    "        h, w, _ = image.shape\n",
    "        \n",
    "        new_h, new_w = self.output_size\n",
    "        \n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "        \n",
    "        image = image[top:top + new_h, left:left + new_w, :]\n",
    "        #label = label[top:top + new_h, left:left + new_w, :]\n",
    "    \n",
    "        return image#, label\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below is prelim octclassification dataset \n",
    "\n",
    "labels_dict is a dict {'name':hard or not?} if hard [0,1] if not [1,0] REMEMBER TO CLASS IMBALANCE HERE\n",
    "\n",
    "keys of label_dict become possible names for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCTClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    First we create a dataset that will encapsulate our data. It has 3 special \n",
    "    functions which will be explained as they go. We will pass this dataset object\n",
    "    to the torch dataloader object later which will make training easier.\n",
    "    \"\"\"\n",
    "    def __init__ (self,\n",
    "                  image_data_dir,\n",
    "                  labels_dict,\n",
    "                  start_size,\n",
    "                  cropped_size,\n",
    "                  transform):\n",
    "        self.image_data_dir = image_data_dir\n",
    "        self.labels_dir = labels_dict \n",
    "        self.start_size = start_size\n",
    "        self.transform = transform\n",
    "        self.cropped_size = cropped_size\n",
    "        \n",
    "        \n",
    "        self.rcrop = RandomSingleImageCrop(self.cropped_size)\n",
    "        self.phflip = np.random.rand()\n",
    "        self.pvflip = np.random.rand()\n",
    "        \n",
    "        #iterate through the 2d images and get all their name\n",
    "            \n",
    "        self.name_list = list(self.labels_dict.keys())\n",
    "    \n",
    "    def visualise(self, idx):\n",
    "        \n",
    "        sample = self.__getitem__(idx)\n",
    "        #print(sample['input'].size())\n",
    "        #print(sample['label'].size())\n",
    "        input_data = sample['input'].cpu().numpy()[0,:,:]\n",
    "        label = sample['label'].cpu().numpy()[0]\n",
    "\n",
    "        \"\"\"\n",
    "        put control flow to get easy or hard depdending on label\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        f, (axin, axl, ax1comb) = plt.subplots(1,3, sharey=True)\n",
    "        f.subplots_adjust(hspace=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        #plot image\n",
    "        image = axin.imshow(input_data,\n",
    "                            aspect = 'equal')\n",
    "        f.colorbar(image, ax=axin, orientation='vertical', fraction = 0.05)\n",
    "        \n",
    "        axl.imshow(l_data,\n",
    "                   aspect = 'equal')\n",
    "        \n",
    "        \n",
    "        combined = input_data + 1 * l_data \n",
    "        \n",
    "        ax1comb.imshow(combined, aspect = 'equal')\n",
    "        plt.show()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"This function will allow us to index the data object and it will \n",
    "        return a sample.\"\"\"\n",
    "        name = self.name_list[idx]\n",
    "        \n",
    "        #load data  \n",
    "        label = self.labels_dict[name]\n",
    "        \n",
    "        image = get_image(self.image_data_dir, name, 'images')\n",
    "        image = image.astype(float)\n",
    "        \n",
    "        \n",
    "        #print(image.shape)\n",
    "        \n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "        #print(label.max())\n",
    "        #print(Image.shape)\n",
    "        if self.transform:\n",
    "            \n",
    "            ysize = self.start_size[0] + 20\n",
    "            xsize = self.start_size[1] + 20\n",
    "            image = skitransforms.resize(image, output_shape=(ysize, xsize))          \n",
    "            \n",
    "            #print(label.shape)\n",
    "            #print(label.max())\n",
    "            image = self.rcrop(image)\n",
    "            #print(label.max())\n",
    "            \n",
    "            if self.phflip>0.5:\n",
    "                #hflip\n",
    "                image = np.flip(image, 1)\n",
    "                #print(label.max())\n",
    "            #print(label.shape)\n",
    "            \n",
    "            if self.pvflip>0.5:\n",
    "                #vflip\n",
    "                image = np.flip(image, 0)\n",
    "                #print(label.max())\n",
    "            #print(label.shape)\n",
    "            \n",
    "            angle = np.random.randint(0,360)\n",
    "            image = skitransforms.rotate(image, angle=angle, mode='reflect')\n",
    "            #print(label.max())\n",
    "            #print(label.shape)\n",
    "            \n",
    "            if np.random.rand() > 0.5:\n",
    "                image = gaussian(image, sigma=1, mode='reflect')\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            image = skitransforms.resize(image, output_shape= self.start_size)\n",
    "        \n",
    "        #image = np.expand_dims(preprocessing.scale(image[:,:,0]), -1)\n",
    "        \n",
    "        image = np.transpose(image.copy(), (2, 0, 1))\n",
    "        #og = preprocessing.MinMaxScaler(og)\n",
    "        \n",
    "        sample = {'input': torch.tensor(image),\n",
    "                  'label': torch.tensor(label),\n",
    "                  'case_name': name}\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):    \n",
    "        \"\"\"This function is mandated by Pytorch and allows us to see how many \n",
    "        data points we have in our dataset\"\"\"\n",
    "        return len(self.name_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init oct data set here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-34-9a4cd739bdcc>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-9a4cd739bdcc>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    octdata = OCTClassificationDataset(blah blah blah!)\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "octdata = OCTClassificationDataset(blah blah blah!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
